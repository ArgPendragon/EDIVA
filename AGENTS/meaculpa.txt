ðŸ“œ Lessons Learned: Debugging AutoGen + OpenRouter Like a Pro
ðŸ”¥ (or: What I Screwed Up & How Future Me Wonâ€™t Do It Again) ðŸ”¥

Debugging AutoGen with OpenRouter was a wild rideâ€”hereâ€™s a self-critical look at what I missed, what I shouldâ€™ve done earlier, and how future me (and anyone else dealing with this) can debug smarter next time.

ðŸ’€ 1. I Trusted AutoGen Too Much at First
ðŸ”´ Mistake:
I assumed AutoGen would correctly forward OpenRouter settings once llm_config was set up. Reality: It silently dropped key parameters, leading to API rejections.

âœ… Lesson Learned:
ðŸ”¹ Never assume AutoGen is correctly forwarding API settingsâ€”always print & verify whatâ€™s actually being sent.

ðŸ”¹ Debug earlier with print(llm_config), instead of assuming itâ€™s formatted correctly.

ðŸ’€ 2. I Didnâ€™t Test OpenRouter API First
ðŸ”´ Mistake:
I started debugging inside AutoGen first, instead of checking if OpenRouter worked independently.

âœ… Lesson Learned:
ðŸ”¹ First rule of debugging: Test the API manually before blaming AutoGen.
ðŸ”¹ A 5-line requests.post() test would've instantly told us if OpenRouter was working.

ðŸ‘‰ Next time: The first step should always be a manual API test.

ðŸ’€ 3. I Didnâ€™t Force OpenAIâ€™s Headers Correctly at First
ðŸ”´ Mistake:
I tried setting headers inside llm_config, but AutoGen doesnâ€™t support that field.

âœ… Lesson Learned:
ðŸ”¹ AutoGen ignores extra keys in llm_config.
ðŸ”¹ Headers must be set globally using httpx.Client.

ðŸ‘‰ Next time: If AutoGen ignores a setting, try setting it globally instead of inside llm_config.

ðŸ’€ 4. I Assumed OpenAIâ€™s API Was Backward-Compatible
ðŸ”´ Mistake:
I didnâ€™t realize OpenAI completely changed its API in version 1.x, breaking .create() calls.

âœ… Lesson Learned:
ðŸ”¹ Whenever OpenAI updates, check breaking changes first.
ðŸ”¹ Run:

powershell
Copia
Modifica
pip show openai
to confirm what version is actually installed.

ðŸ‘‰ Next time: Before assuming the bug is in AutoGen, check if OpenAI itself changed.

ðŸ’€ 5. I Didnâ€™t Print Debug Info Early Enough
ðŸ”´ Mistake:
We wasted time guessing when we could have printed key settings immediately.

âœ… Lesson Learned:
ðŸ”¹ Print API keys (masked), URLs, headers, and requests early to see if things are being passed correctly.
ðŸ”¹ Debugging starts with checking what's actually being sent, not what's in the script.

ðŸ‘‰ Next time: Print everything important before the first request.

ðŸ’€ 6. PowerShellâ€™s curl Is Garbage for Testing APIs
ðŸ”´ Mistake:
Told you to run:

powershell
Copia
Modifica
curl -H "Authorization: Bearer sk-or-v1-..." -X POST https://openrouter.ai/api/v1/chat/completions
â€¦but PowerShellâ€™s curl is actually Invoke-WebRequest, which needs different syntax.

âœ… Lesson Learned:
ðŸ”¹ Use Python (requests) instead of PowerShell for API testing.
ðŸ”¹ If using PowerShell, run:

powershell
Copia
Modifica
Invoke-RestMethod -Uri "https://openrouter.ai/api/v1/chat/completions" -Headers @{"Authorization"="Bearer sk-or-v1-..."} -Method POST
ðŸ‘‰ Next time: Stick to Python for API testsâ€”PowerShell's syntax is too annoying.

ðŸ’€ 7. I Shouldâ€™ve Checked AutoGenâ€™s Error Messages Sooner
ðŸ”´ Mistake:
AutoGen does log errors, but I didnâ€™t check them early enough.

âœ… Lesson Learned:
ðŸ”¹ Use AutoGenâ€™s logs (autogen.oai.client) to see whatâ€™s happening.
ðŸ”¹ Example:

python
Copia
Modifica
import logging
logging.basicConfig(level=logging.DEBUG)
This forces AutoGen to show more detailed logs.

ðŸ‘‰ Next time: Before debugging settings, check AutoGenâ€™s own logs.


ðŸš€ Summary of What We Learned
Issue	Lesson Learned
Missing fields (api_key, description, model_client_stream)	Check required fields in JSON configs
API key issues	Use environment variables instead of hardcoding API keys
Model selection impact	Choose gpt-4o-mini for cost, gpt-4o for reasoning
model_client_stream missing	Always set "model_client_stream": false
Premature termination	Ensure termination conditions do not block responses
Incorrect stop format	Use "stop": ["TERMINATE"] without extra brackets
Tool execution untested	Explicitly test tools before assuming they work