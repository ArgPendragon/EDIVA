📜 Lessons Learned: Debugging AutoGen + OpenRouter Like a Pro
🔥 (or: What I Screwed Up & How Future Me Won’t Do It Again) 🔥

Debugging AutoGen with OpenRouter was a wild ride—here’s a self-critical look at what I missed, what I should’ve done earlier, and how future me (and anyone else dealing with this) can debug smarter next time.

💀 1. I Trusted AutoGen Too Much at First
🔴 Mistake:
I assumed AutoGen would correctly forward OpenRouter settings once llm_config was set up. Reality: It silently dropped key parameters, leading to API rejections.

✅ Lesson Learned:
🔹 Never assume AutoGen is correctly forwarding API settings—always print & verify what’s actually being sent.

🔹 Debug earlier with print(llm_config), instead of assuming it’s formatted correctly.

💀 2. I Didn’t Test OpenRouter API First
🔴 Mistake:
I started debugging inside AutoGen first, instead of checking if OpenRouter worked independently.

✅ Lesson Learned:
🔹 First rule of debugging: Test the API manually before blaming AutoGen.
🔹 A 5-line requests.post() test would've instantly told us if OpenRouter was working.

👉 Next time: The first step should always be a manual API test.

💀 3. I Didn’t Force OpenAI’s Headers Correctly at First
🔴 Mistake:
I tried setting headers inside llm_config, but AutoGen doesn’t support that field.

✅ Lesson Learned:
🔹 AutoGen ignores extra keys in llm_config.
🔹 Headers must be set globally using httpx.Client.

👉 Next time: If AutoGen ignores a setting, try setting it globally instead of inside llm_config.

💀 4. I Assumed OpenAI’s API Was Backward-Compatible
🔴 Mistake:
I didn’t realize OpenAI completely changed its API in version 1.x, breaking .create() calls.

✅ Lesson Learned:
🔹 Whenever OpenAI updates, check breaking changes first.
🔹 Run:

pip show openai
to confirm what version is actually installed.

👉 Next time: Before assuming the bug is in AutoGen, check if OpenAI itself changed.

💀 5. I Didn’t Print Debug Info Early Enough
🔴 Mistake:
We wasted time guessing when we could have printed key settings immediately.

✅ Lesson Learned:
🔹 Print API keys (masked), URLs, headers, and requests early to see if things are being passed correctly.
🔹 Debugging starts with checking what's actually being sent, not what's in the script.

👉 Next time: Print everything important before the first request.

💀 6. PowerShell’s curl Is Garbage for Testing APIs
🔴 Mistake:
Told you to run:


curl -H "Authorization: Bearer sk-or-v1-..." -X POST https://openrouter.ai/api/v1/chat/completions
…but PowerShell’s curl is actually Invoke-WebRequest, which needs different syntax.

✅ Lesson Learned:
🔹 Use Python (requests) instead of PowerShell for API testing.
🔹 If using PowerShell, run:

powershell
Invoke-RestMethod -Uri "https://openrouter.ai/api/v1/chat/completions" -Headers @{"Authorization"="Bearer sk-or-v1-..."} -Method POST
👉 Next time: Stick to Python for API tests—PowerShell's syntax is too annoying.

💀 7. I Should’ve Checked AutoGen’s Error Messages Sooner
🔴 Mistake:
AutoGen does log errors, but I didn’t check them early enough.

✅ Lesson Learned:
🔹 Use AutoGen’s logs (autogen.oai.client) to see what’s happening.
🔹 Example:

import logging
logging.basicConfig(level=logging.DEBUG)
This forces AutoGen to show more detailed logs.

👉 Next time: Before debugging settings, check AutoGen’s own logs.


🚀 Summary of What We Learned
Issue	Lesson Learned
Missing fields (api_key, description, model_client_stream)	Check required fields in JSON configs
API key issues	Use environment variables instead of hardcoding API keys
Model selection impact	Choose gpt-4o-mini for cost, gpt-4o for reasoning
model_client_stream missing	Always set "model_client_stream": false
Premature termination	Ensure termination conditions do not block responses
Incorrect stop format	Use "stop": ["TERMINATE"] without extra brackets
Tool execution untested	Explicitly test tools before assuming they work

🔪 AutoGen Debugging Kill Bill List
(aka WTF Went Wrong and How We Fixed It)

☠️ The List
1️⃣ API Key Not Found (401 Error) – 🔥 FIXED
✅ Passed api_key inside extra_create_args instead of assuming it's a direct attribute.

2️⃣ OpenRouter Timeout (No Response) – 🔥 FIXED
✅ Verified OpenRouter is up (ping test worked).
✅ PowerShell API call worked (Invoke-RestMethod returned models).
✅ Issue likely inside AutoGen, not OpenRouter.

3️⃣ OpenAIChatCompletionClient Missing 'model' – 🔥 FIXED
✅ Added model="gpt-4o" when initializing OpenAIChatCompletionClient.

4️⃣ 'api_key' Attribute Missing – 🔥 FIXED
✅ Used model_client._create_args.get('api_key') instead of model_client.api_key.
✅ Printed masked key to verify it's being read.

5️⃣ Debugging Info Not Showing in Logs – 🔥 FIXED
✅ Injected logging.basicConfig(level=logging.DEBUG).
✅ Wrapped OpenRouter request in debug_request() before sending it.

Ecco un riassunto di ciò che abbiamo imparato da questa lunga sessione di debug:

Interfacce Instabili e Interni Cambiati

OpenAI Client e Headers:
Abbiamo provato a "monkey-patch" il client OpenAI per forzare la presenza dell’attributo headers, ma la struttura interna della libreria (ad esempio, la posizione e la disponibilità della classe Client) sembra essere cambiata. Gli errori come
module 'openai._base_client' has no attribute 'Client'
e
'OpenAI' object has no attribute 'headers'
indicano che l’API interna non corrisponde alle nostre aspettative. Questo significa che, per funzionare con OpenRouter, occorre una strategia diversa o aggiornare la versione della libreria, oppure adattare il nostro patching in base al nuovo schema interno.

Differenze tra Modalità Singolo Agente e Team

Singolo Agente:
Il codice per la modalità singolo agente (con AssistantAgent e UserProxyAgent) funziona per quanto riguarda la configurazione delle variabili e la connessione all’API (almeno fino a quando non si incontra il problema degli header). Il problema principale rimane nel fatto che la libreria OpenAI non espone l’attributo headers come ci aspettavamo.

Team Chat:
La configurazione del team, derivata dal JSON, includeva campi che il costruttore del team non accetta (come initial_messages e persino il passaggio diretto di configurazioni che contengono il parametro llm_config). Abbiamo dovuto:

Rimuovere manualmente chiavi non supportate.
Convertire la lista dei partecipanti (inizialmente definita come dizionari) in vere istanze di AssistantAgent, poiché il team si aspetta oggetti dotati di attributi (ad esempio, il campo name).
Notare che, in modalità team, passare llm_config direttamente a AssistantAgent non è accettato, evidenziando che la modalità team potrebbe richiedere una configurazione diversa o che le API per la creazione degli agenti sono cambiate.
Integrazione con OpenRouter

Abbiamo confermato che le variabili d’ambiente (come OPENROUTER_API_KEY e OPENROUTER_API_BASE) vengono lette correttamente e che, almeno in parte, la configurazione è corretta.
Tuttavia, il collegamento tra la libreria OpenAI (modificata internamente) e il client utilizzato da autogen presenta ancora dei problemi non risolti, in particolare per quanto riguarda la gestione degli header nelle richieste.
Lezione di Fondamenta:

Le librerie sperimentali come autogen e relative integrazioni (OpenRouter, OpenAI) possono cambiare rapidamente. Le interfacce interne non sempre sono stabili e, quando si fa debug, occorre spesso “scavare” nel codice sorgente per capire dove e come intervenire.
La configurazione in modalità team richiede un adattamento più “manuale” (conversione dei dizionari di configurazione in oggetti) rispetto a quella per il singolo agente.
In sintesi:
Abbiamo capito che la nostra integrazione è ostacolata da due grandi ostacoli:

La struttura interna della libreria OpenAI non corrisponde alle nostre ipotesi (problemi con l'attributo headers e con il patching del client).
La configurazione del team in autogen richiede la trasformazione dei dati (dizionari) in oggetti reali e la rimozione di parametri non supportati (come llm_config), indicando che la modalità team ha un'API diversa da quella del singolo agente.
Questi sono i punti di sostanza che ci hanno aiutato a comprendere dove intervenire per rendere il sistema funzionante. Sembra che dovremo rivedere l'approccio di patching per OpenAI e ripensare il modo in cui istanziamo e configuriamo i partecipanti nel team.

Utilizzo delle API Asincrone
Il codice è completamente asincrono:

Viene definito un tool (la funzione get_weather) che viene integrato come funzionalità dell’agente.
L’interazione con l’utente avviene in un loop asincrono (usando await Console(stream) per visualizzare lo streaming delle risposte).
Alla fine, il main viene eseguito con await main(), il che è particolarmente utile se usi un ambiente che supporta l’esecuzione di coroutine (oppure, se usi uno script standalone, potresti dover avviare il main con asyncio.run(main())).
Integrazione dei Componenti Offerti da AgentChat
Il quickstart mostra come combinare in modo “plug-and-play” vari componenti:

AssistantAgent: l’agente che gestisce la conversazione.
RoundRobinGroupChat: il team (anche se in questo esempio c’è un solo agente, la logica round-robin è già integrata) per gestire il flusso di conversazione.
Console: una semplice interfaccia utente per interagire da terminale.
OpenAIChatCompletionClient: il client che interfaccia il modello OpenAI (in questo esempio viene specificato il modello "gpt-4o-2024-08-06", ma naturalmente puoi sostituirlo con quello che preferisci).
Modularità e Facilità d’Uso
Il codice mostra come, grazie a autogen-agentchat e autogen-ext, è possibile configurare e far girare rapidamente un’applicazione basata su agenti. Basta definire l’agente, specificare gli strumenti (tool) e comporre il team: il framework si occupa del resto, gestendo il flusso conversazionale in modo automatico.

----
In other words, we want the SDK to use a custom HTTP client that forces all the required headers 
----